{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION AND CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression: \n",
    "Measure of relationship between mean value of one variable with corresponding value(s) of (an) other variable(s). Unlike classification (discussed below), regression models continuous/ordered values.\n",
    "In regression, we evaluate our model based on how close the predicted value is to the actual value of an observation. \n",
    "\n",
    "## Classification:\n",
    "Process of determining the category or class that an observation belongs to.\n",
    "Can be binary (spam or no spam) or multiclass (which one of \"n\" classes, does an observation belong to?).\n",
    "Classification is a supervised learning technique: we need to have a dataset with correct class labels for training. When correct class labels are not available, the corresponding unsupervised learning technique we can use is clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1:  Process and generate the initial data set for modeling. \n",
    "It will contain both the predictor variables (x1...xn) and the target variable y (Y is called the target variable because the supervised learning's \"goal\" is to is predict the missing variable y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Split the data into training and testing sets for evaluation of our prediction model. \n",
    "\n",
    "Q: Why split? Why can't we evaluate our model on the training data itself? \n",
    "A: Because then, we'd have a very optimistic view of our models performance, since it was optimized directly on the training data.\n",
    "\n",
    "The split needs to be performed in such a way that both sets contain the same patterns of data. For example, if the initial data set has 100 observations, with the predictor value being 60 \"Y\" and 40 \"N\" in these observations, the individual training and testing sets should also have the Y and N in the same ratio of 3:2. \n",
    "\n",
    "Splitting is usually done through random selection.\n",
    "\n",
    "Typically, the training set has 70% of the records in the initial data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: Build a model on the training data set.\n",
    "\n",
    "Building a model (also called training or fitting) boils down to optimizing a set of parameters on the training set. For instance, in the case of linear regression, we assume a linear relationship between our features and our response variable, and learn the coefficients of the model from our training set. This occurs through minimizing our squared error loss function:\n",
    "\n",
    "Loss(*beta*) = **1/n** x (summation over i) (**y_i** - [**X_i** x *beta*])^2, with respect to each*beta* parameter (i.e., taking the partial derivatives and setting them to zero).  \n",
    "\n",
    "Avoid overfitting models (learning the training data too closely; otherwise, this would cause the prediction of new data to perform more poorly). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4: Test the model using the test data set. \n",
    "We use the model to predict the value of Y in each of the test records. Then compare the predicted value of Y with the actual value of Y. This gives an indication of the accuracy of the prediction model built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 5: Fine tune the model using cross-validation. \n",
    "Try different algorithms/parameters, until the accuracy reaches an acceptable level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 6: Predict unknown data.\n",
    "Use the model built on new data (where Y is NOT known) to predict Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of Overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set seed for reproducible results\n",
    "np.random.seed(414)\n",
    "\n",
    "# Gen toy data\n",
    "X = np.linspace(0, 15, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = 3 * np.sin(X) + np.random.normal(1 + X, .2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_y = X[:700], y[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X, test_y = X[700:], y[700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             X         y\n",
      "0     0.000000  1.015209\n",
      "1     0.015015  0.980087\n",
      "2     0.030030  1.169571\n",
      "3     0.045045  1.269522\n",
      "4     0.060060  1.258568\n",
      "5     0.075075  1.451896\n",
      "6     0.090090  1.222422\n",
      "7     0.105105  1.301232\n",
      "8     0.120120  1.459951\n",
      "9     0.135135  1.236465\n",
      "10    0.150150  1.416206\n",
      "11    0.165165  1.712582\n",
      "12    0.180180  1.674174\n",
      "13    0.195195  1.481929\n",
      "14    0.210210  1.789626\n",
      "15    0.225225  1.702812\n",
      "16    0.240240  1.907079\n",
      "17    0.255255  2.017517\n",
      "18    0.270270  2.184883\n",
      "19    0.285285  1.975227\n",
      "20    0.300300  2.270087\n",
      "21    0.315315  2.271371\n",
      "22    0.330330  2.238102\n",
      "23    0.345345  2.097258\n",
      "24    0.360360  2.191813\n",
      "25    0.375375  2.312131\n",
      "26    0.390390  2.929569\n",
      "27    0.405405  2.387823\n",
      "28    0.420420  2.774248\n",
      "29    0.435435  2.873006\n",
      "..         ...       ...\n",
      "670  10.060060  9.189485\n",
      "671  10.075075  8.839575\n",
      "672  10.090090  9.301995\n",
      "673  10.105105  9.089993\n",
      "674  10.120120  9.122525\n",
      "675  10.135135  9.437679\n",
      "676  10.150150  9.290701\n",
      "677  10.165165  9.264488\n",
      "678  10.180180  9.092215\n",
      "679  10.195195  9.118881\n",
      "680  10.210210  9.220211\n",
      "681  10.225225  8.888530\n",
      "682  10.240240  9.242559\n",
      "683  10.255255  8.834287\n",
      "684  10.270270  8.975822\n",
      "685  10.285285  9.060579\n",
      "686  10.300300  8.852828\n",
      "687  10.315315  9.038193\n",
      "688  10.330330  9.218815\n",
      "689  10.345345  9.318012\n",
      "690  10.360360  8.598440\n",
      "691  10.375375  9.002141\n",
      "692  10.390390  9.312440\n",
      "693  10.405405  8.859884\n",
      "694  10.420420  9.031436\n",
      "695  10.435435  8.956333\n",
      "696  10.450450  8.978750\n",
      "697  10.465465  8.892355\n",
      "698  10.480480  8.774992\n",
      "699  10.495495  9.020233\n",
      "\n",
      "[700 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame({'X': train_X, 'y': train_y})\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             X          y\n",
      "0    10.510511   9.113557\n",
      "1    10.525526   8.798065\n",
      "2    10.540541   8.931348\n",
      "3    10.555556   9.047734\n",
      "4    10.570571   8.659095\n",
      "5    10.585586   9.115196\n",
      "6    10.600601   8.966861\n",
      "7    10.615616   8.947426\n",
      "8    10.630631   8.761166\n",
      "9    10.645646   8.570319\n",
      "10   10.660661   8.542211\n",
      "11   10.675676   8.840849\n",
      "12   10.690691   8.685590\n",
      "13   10.705706   9.116769\n",
      "14   10.720721   9.013263\n",
      "15   10.735736   8.645331\n",
      "16   10.750751   8.956652\n",
      "17   10.765766   8.911172\n",
      "18   10.780781   8.496090\n",
      "19   10.795796   8.867917\n",
      "20   10.810811   8.936554\n",
      "21   10.825826   8.829840\n",
      "22   10.840841   9.052438\n",
      "23   10.855856   8.630678\n",
      "24   10.870871   8.828351\n",
      "25   10.885886   8.995500\n",
      "26   10.900901   8.735839\n",
      "27   10.915916   9.041912\n",
      "28   10.930931   8.613124\n",
      "29   10.945946   8.498318\n",
      "..         ...        ...\n",
      "270  14.564565  18.319723\n",
      "271  14.579580  18.164795\n",
      "272  14.594595  18.381468\n",
      "273  14.609610  18.050854\n",
      "274  14.624625  18.022420\n",
      "275  14.639640  18.263179\n",
      "276  14.654655  18.530608\n",
      "277  14.669670  18.346488\n",
      "278  14.684685  18.368021\n",
      "279  14.699700  18.127846\n",
      "280  14.714715  18.329376\n",
      "281  14.729730  18.223719\n",
      "282  14.744745  18.408968\n",
      "283  14.759760  18.461415\n",
      "284  14.774775  18.273132\n",
      "285  14.789790  18.052979\n",
      "286  14.804805  18.096345\n",
      "287  14.819820  17.917506\n",
      "288  14.834835  18.280223\n",
      "289  14.849850  18.105073\n",
      "290  14.864865  18.365080\n",
      "291  14.879880  18.380153\n",
      "292  14.894895  18.046442\n",
      "293  14.909910  17.777680\n",
      "294  14.924925  18.202361\n",
      "295  14.939940  17.680874\n",
      "296  14.954955  17.963930\n",
      "297  14.969970  18.089490\n",
      "298  14.984985  17.919929\n",
      "299  15.000000  18.132281\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'X': test_X, 'y': test_y})\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.642</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.642</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1254.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 30 Sep 2015</td> <th>  Prob (F-statistic):</th> <td>5.52e-158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:27:39</td>     <th>  Log-Likelihood:    </th> <td> -1483.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   700</td>      <th>  AIC:               </th> <td>   2971.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   698</td>      <th>  BIC:               </th> <td>   2980.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.9959</td> <td>    0.152</td> <td>   13.104</td> <td> 0.000</td> <td>    1.697     2.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X</th>         <td>    0.8896</td> <td>    0.025</td> <td>   35.405</td> <td> 0.000</td> <td>    0.840     0.939</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>701.108</td> <th>  Durbin-Watson:     </th> <td>   0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  52.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.259</td>  <th>  Prob(JB):          </th> <td>3.13e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.756</td>  <th>  Cond. No.          </th> <td>    12.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.642\n",
       "Model:                            OLS   Adj. R-squared:                  0.642\n",
       "Method:                 Least Squares   F-statistic:                     1254.\n",
       "Date:                Wed, 30 Sep 2015   Prob (F-statistic):          5.52e-158\n",
       "Time:                        11:27:39   Log-Likelihood:                -1483.4\n",
       "No. Observations:                 700   AIC:                             2971.\n",
       "Df Residuals:                     698   BIC:                             2980.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.9959      0.152     13.104      0.000         1.697     2.295\n",
       "X              0.8896      0.025     35.405      0.000         0.840     0.939\n",
       "==============================================================================\n",
       "Omnibus:                      701.108   Durbin-Watson:                   0.020\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               52.980\n",
       "Skew:                          -0.259   Prob(JB):                     3.13e-12\n",
       "Kurtosis:                       1.756   Cond. No.                         12.4\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Fit\n",
    "poly_1 = smf.ols(formula='y ~ 1 + X', data=train_df).fit()\n",
    "poly_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.666</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.665</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   694.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 30 Sep 2015</td> <th>  Prob (F-statistic):</th> <td>1.25e-166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:28:09</td>     <th>  Log-Likelihood:    </th> <td> -1459.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   700</td>      <th>  AIC:               </th> <td>   2925.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   697</td>      <th>  BIC:               </th> <td>   2939.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    3.1458</td> <td>    0.221</td> <td>   14.261</td> <td> 0.000</td> <td>    2.713     3.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X</th>         <td>    0.2313</td> <td>    0.097</td> <td>    2.382</td> <td> 0.017</td> <td>    0.041     0.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(X ** 2)</th> <td>    0.0627</td> <td>    0.009</td> <td>    7.004</td> <td> 0.000</td> <td>    0.045     0.080</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1210.467</td> <th>  Durbin-Watson:     </th> <td>   0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  49.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.091</td>  <th>  Prob(JB):          </th> <td>1.45e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 1.705</td>  <th>  Cond. No.          </th> <td>    160.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.666\n",
       "Model:                            OLS   Adj. R-squared:                  0.665\n",
       "Method:                 Least Squares   F-statistic:                     694.4\n",
       "Date:                Wed, 30 Sep 2015   Prob (F-statistic):          1.25e-166\n",
       "Time:                        11:28:09   Log-Likelihood:                -1459.6\n",
       "No. Observations:                 700   AIC:                             2925.\n",
       "Df Residuals:                     697   BIC:                             2939.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      3.1458      0.221     14.261      0.000         2.713     3.579\n",
       "X              0.2313      0.097      2.382      0.017         0.041     0.422\n",
       "I(X ** 2)      0.0627      0.009      7.004      0.000         0.045     0.080\n",
       "==============================================================================\n",
       "Omnibus:                     1210.467   Durbin-Watson:                   0.022\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               49.911\n",
       "Skew:                          -0.091   Prob(JB):                     1.45e-11\n",
       "Kurtosis:                       1.705   Cond. No.                         160.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quadratic Fit\n",
    "poly_2 = smf.ols(formula='y ~ 1 + X + I(X**2)', data=train_df).fit()\n",
    "poly_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and Cross-Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a supervised model, it's important to have a clear idea of how you will evaluate its performance. As we've discussed, one crucial part of evaluation is to separate your training and testing set to avoid overfitting. Once we have done this, it's time to decide exactly what metric we will use to evaluate our model. Metrics differ between regression and classification models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean Squared Error: Mean square error between our predicted outcomes, and the true response in our test set.\n",
    "\n",
    "2. Mean Absolute Error: Mean absolute error between our predicted outcomes, and the true response in our test set.\n",
    "\n",
    "3. R-Squared: Coefficient of determination from regression score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Accuracy: The percentage of data points labeled correctly by the model.\n",
    "\n",
    "5. Precision: The ratio of (true positives / (true_positives + true_negatives)). The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "6. Recall: The ratio of (true_positives / (true_positives + false_negatives)). The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "A typical work flow while building a model is to split your data, build your model, and then choose a metric to evaluate it.For instance, suppose we wanted to build a classifier to predict whether or not an individual had cancer. In this scenario, the harm of incorrectly predicting that some one does not have cancer who actually does is far worse than predicting that a healthy person has cancer. Thus, we want to minimize our false negatives, and might choose recall as our metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
